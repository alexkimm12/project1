---
title: 'Project 1: Wrangling, Exploration, Visualization'
author: "SDS322E"
date: ''
output:
  html_document:
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
  pdf_document:
    toc: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, fig.align = "center", warning = F, message = F,
tidy=TRUE, tidy.opts=list(width.cutoff=60), R.options=list(max.print=100))
```

## Data Wrangling, Exploration, Visualization

### Alex Kim UT Eid: ak37642

#### Introduction 

Paragraph or two introducing your datasets and variables, why they are interesting to you, etc.

```{R}
# read your datasets in here, e.g., with read_csv()
library(tidyverse)
library(readr)
library(fivethirtyeight)
data1 <- bad_drivers
data2 <- read_csv("~/project1/2021 County Health Rankings Data - v1.csv", skip = 1)
```
I chose the bad_drivers dataset from the fivethirtyeight package and the county health ratings from Countyhealthratings.org (under the Rankings data and document page). I wanted to look at these datasets because I am very big on keeping up with the news. One thing that I always see are news articles on drunk driving, speeding, or just not paying attention lead to people losing their lives due to these accidents. I know that these are just a few factors; therefore, I wanted to bring in the County health rankings data because the data provides more insight into the health aspect of these various counties in each state of the United States.

Some of the variables in my first dataset of "bad_drivers" include number of drivers involved in fatal collisions, percentage of those speeding, alcohol-impaired, not distracted, no prior accidents before this collision, cost of car insurance, and losses experienced by insurance companies across all 50 states. Furthermore, some of the variables for my second dataset of "County_Health_Rankings" include life expectancy, deaths, percent of ethnicities, number of people insured, and more across counties in the 50 states in the United States.I would expect to see that if there were more traffic accidents the location and how populated the state is, could impact the number of traffic fatalities. Furthermore, if there was an increase in the number of people uninsured, this could influence how the insurance premium will come out to be. 

#### Joining/Merging

```{R}
#Joining the datasets
joineddata <- full_join(data1, data2, by=c("state"="State"))

# Finding which is missing from each dataset
not_in_data1 <- anti_join(data1, data2, by=c("state"="State"))
not_in_data2 <- anti_join(data2, data1, by=c("State"="state"))
```
I combined both of my datasets by the common variable of "states." To do this, I utilized the full_join function to retain all of my rows, which is why there was 3,193 rows, one row for each county in each state. Furthermore, the two datasets were joined by the common ID of "state." When fully joined, there were 3,193 rows with 72 columns. Before the full join, the bad_drivers dataset had 51 rows of the 50 states in the U.S. as well as D.C. with 8 variables. Also before the join, the County health ratings data had 3,193 rows, for each county in the each state of the U.S. along with 65 variables. There were 51 unique IDs, or states, for both the bad_drivers and county health rating datasets. Since both had only unique IDs of the 51 states, there were no IDs that were different from each other in both datasets. Since both datasets were joined by the common function of states, all 50 states in the United States including D.C. were included in both datasets. No observations were dropped and were actually retained by the full join function as well as the common ID being the same for each dataset. The NAs that are located in the full join and extra columns will be tidied. which will be shown above in the tidying section.  

#### Tidying: Reshaping

If your datasets are tidy already, demonstrate that you can reshape data with pivot wider/longer here (e.g., untidy and then retidy). Alternatively, it may be easier to wait until the wrangling section so you can reshape your summary statistics. Note here if you are going to do this.

```{R}
# your tidying code (if applicable; can also wait until wrangling section)
clean_joineddata <- joineddata %>% select(`state`, `num_drivers`, `perc_alcohol`, `insurance_premiums`, `County`, `Life Expectancy`, `# Deaths`, `% Frequent Physical Distress`, `# Uninsured`, `Median Household Income`, `Homicide Rate`, `# Asian`, `# American Indian & Alaska Native`, `# Native Hawaiian/Other Pacific Islander`, `# Hispanic`, `# Black`, `# Non-Hispanic White`, `% Rural`)

clean_joineddata <- clean_joineddata %>% rename("DriverDeaths"=num_drivers, "AlcoholRelated"=perc_alcohol,"LE"=`Life Expectancy`, "Deaths"=`# Deaths`, "PhysicalDistress"=`% Frequent Physical Distress`, "Uninsured"=`# Uninsured`, "HouseholdIncome"=`Median Household Income`, "Homicide"=`Homicide Rate`, "Asian"=`# Asian`, "AmericanIndianAlaskaNative"=`# American Indian & Alaska Native`, "PacificIslander"=`# Native Hawaiian/Other Pacific Islander`, "Hispanic"=`# Hispanic`, "Black"=`# Black`, "White"=`# Non-Hispanic White`, "PercRural"=`% Rural`, "Premiums"=`insurance_premiums`)

clean_joineddata <- clean_joineddata %>% group_by(state) %>% summarize_at(c("LE", "Deaths", "DriverDeaths", "AlcoholRelated", "PhysicalDistress", "Uninsured", "HouseholdIncome", "Homicide", "PercRural", "Premiums", "Asian", "AmericanIndianAlaskaNative", "PacificIslander", "Hispanic", "Black", "White"), .funs=list(mean=mean)) 

clean_joineddata2 <- clean_joineddata %>% pivot_longer(-1) %>% separate(name, into=c("name", "stat")) %>% pivot_wider(names_from="name", values_from="value") %>% select(-stat) %>% mutate(Rural=ifelse(`PercRural`>50, "yes", "no"))
```
Since there were so many different variables, I wanted to just select 10 different unique variables and then variables of different ethnicities. I wanted to untidy and tidy my data first before I started on the wrangling portion, and I just wanted to clean up the names before moving on. There were over 72 columns with the data fully joined. In this case, I had to first select the columns I wanted and then remove the extraneous ones like percent of the population over 65, graduation rate, GPA, child mortality rate, infant mortality, etc. With this, I made a cleaner version of the joineddata (clean_joineddata). I then wanted to just make to create a new categorical variable of whether one state would be considered rural or not. Since the variable Rural was in percentages, I mutated the variable with the ifelse function to say that if the average 'PercRural' was greater than 50%, then we can say that it is considered rural. If not, then it wasn't considered rural. I then renamed all of the columns because I was having difficulty when it came to tidying the data with the special characters like %, _, and # signs. I then used the pivot_longer and pivot_wider functions to show that I know how to tidy. Since htere were so many repeats of states because of the various counties, I went ahead and actually made new columnns using pivot_longer and pivot_wider to reshape my data to show the averages across the states instead. With these, I found the means of each state with my numeric variables, including one where I mutated the function '% Rural' to give me a yes if the percentage of rural areas was greater than 50%, and no if otherwise. Through this, I was able to get better visualizations of my data because the plots looked really messy otherwise.  

####  Wrangling
```{R}
#Counting NAs
clean_joineddata2 %>% summarize_all(function(x) sum(is.na(x)))
```
When I used the summary function, I found that there was 17 NAs in Life Expectancy, 14 NAs in Deaths, 48 NAs in homicide rates, 2 NAs in percent of counties that are rural, 1 NA in number of people uninsured and median household income, 0 NAs in state, Driver fatalities, Alcohol related traffic fatalities, physical distress, insurance premiums, number of Asians,  American Indian & Alaska Native, Pacific Islander, Hispanic, Black, and White.
```{R}
# % of BIPOC in each county
clean_joineddata2 <- clean_joineddata2 %>% group_by(state) %>% mutate(`% BIPOC`=(sum( `Asian`, `AmericanIndianAlaskaNative`, `PacificIslander`, `Hispanic`, `Black`))/(sum(`Asian`, `AmericanIndianAlaskaNative`, `PacificIslander`, `Hispanic`, `Black`,`White` ))*100)

clean_joineddata2 <- clean_joineddata2 %>% mutate(Diverse=ifelse(`% BIPOC`>50, "high", "low"))

clean_joineddata2 %>% group_by(state) %>% summarize(PercBIPOC=mean(`% BIPOC`)) %>% arrange(desc(PercBIPOC))
```
First, I wanted to create a new variable that would tell us the percentage of many people of color make up the population in each state. To do this, I utilized the mutate function to sum up BIPOC populations, dividing it by the sum of the "BIPOC" populations and "White" populations, and then multiplying by 100. This gave me the percentage of BIPOC in each state within the U.S. I then utilized mutate again and the ifelse function to tell me if these states were considered diverse. If their percentages were greater than 50% then I considered it as high. Lastly, I then grouped by states and utilized the summarize function to find the percentages of BIPOC in each state, and arranged by descending values. I found that Hawaii had the highest % of BIPOC with around 73.716% BIPOC while Maine had the lowest % of 5.512% BIPOC. 

```{R}
# Summary statistics for 10 numeric variables
clean_joineddata2 %>% summarize_at(c("DriverDeaths", "AlcoholRelated", "Premiums", "LE", "Deaths", "PhysicalDistress", "Uninsured", "HouseholdIncome", "Homicide", "PercRural", "Asian", "AmericanIndianAlaskaNative", "PacificIslander", "Hispanic", "Black", "White"), na.rm=TRUE, .funs=list(mean=mean, sd=sd, max=max, min=min, n=n_distinct)) %>% pivot_longer(contains("_")) %>% separate(name, into=c("variable", "stat")) %>% pivot_wider(names_from="variable", values_from="value") %>% knitr::kable() 

clean_joineddata2 %>% summary(is.numeric)

clean_joineddata2 %>% group_by(state) %>% summarize(count=n())
clean_joineddata2 %>% group_by(Rural) %>% summarize(count=n())
clean_joineddata2 %>% group_by(Diverse) %>% summarize(count=n())
```
This is a table of my numeric variables, which shows the mean, standard deviation, maximum, minimum, and distinct counts as well as the five number summary for each state in the United States. This is one big table that prints out these values for the states. I then found the counts for each of my categorical variables. As expected, there is only one count for each of the states. Surprisingly, the amount of states that were considered to be rural were almost even (with 20 being no and 31 being yes). Furthermore, very surprising to see that there were only 6 states that were considered to have high diversity (having a percentage of BIPOC greater than 50). 
```{R}
# Some more summary statistics for my variables
clean_joineddata2 %>% group_by(state, Diverse) %>% summarize(HouseholdIncome=mean(HouseholdIncome)) %>% arrange(desc((HouseholdIncome)))
```
I wanted to compute some extra summary statistics for some of my variables. I grouped by state and Diverse, summarized to find the mean household incomes within each state, and then arranged by descending income. It was very interesting to see that the mean income in D.C. was $90395.00 and the diversity was considered to be high. The lowest average household income was in Mississippi with $41753.00 and the diversity level was considered low. 

```{R}
clean_joineddata2 %>% filter(Rural=="yes") %>% filter(str_detect(state, "^[aeiouAEIOU]"))
```
Lastly, I wanted to filter and find the states that were considered to be rural. This gave me a value of 31, which meant out of the 50 states, we could say that around 31 were considered to be rural with an average percentage of their counties being greater than 50%. I then used the str_detect function to find the states out of the 31 that started with a vowel. This led me to Alabama, Alaska, Arkansas, Colorado, Georgia, Idaho, Illinois, Indiana, Iowa, and Kansas. 

#### Visualizing

```{R}
# Plot of driver fatalities vs. median household income
clean_joineddata2 %>% ggplot(aes(x=DriverDeaths, y=`HouseholdIncome`))+geom_point(aes(color=state))+geom_smooth(method="lm", color=1)+scale_x_continuous(breaks=seq(0,25,5))+scale_y_continuous(breaks=seq(0,100000,10000))+ggtitle("Driver Fatalities vs. Median Income")+xlab("Driver Fatalities")+ylab("Median Income")+theme(legend.position = "none")
```
For the first ggplot, we're looking at the relationship between the number of drivers that have died from a car accident and median household income in each of the 50 U.S. states. Furthermore, i added points that are colored by state to help visualize the differences in each state for driver fatalities and median income. When looking at this plot, we can tell that there seems to be a relatively strong relationship between median household income and driver fatalities. As the median household income in each state decreased, there was a greater number of traffic fatalities that occurred. This could be from various different factors, but I thought it was interesting to see this possible relationship. 

```{R}
# Plot of Driver Deaths vs. Rural Areas
clean_joineddata2 %>% filter(!is.na(Rural)) %>% ggplot(aes(x=Rural, y=DriverDeaths))+geom_bar(aes(fill=Rural), stat="summary", fun=mean)+ggtitle("Driver Deaths vs. Rural Areas")+geom_errorbar(stat="summary", width=0.5)+xlab("Rural Areas")+ylab("Driver Deaths")+scale_fill_manual(values=c("pink", "light blue"))+scale_y_continuous(breaks=seq(0,20,5))+geom_jitter(alpha=0.5, aes(color=Diverse))
```
For my last plot, I wanted to take a look at the number of driver deaths in rural areas with an overlay of Diversity on the points. This was very interesting because we see that in cases of rural states, there is very lower diversity. Both rural and not rural have relatively similar error bars. In this case, rural areas seemed to have a higher amount of driver fatalities than states that weren't considered to be rural. In cases that aren't rural, there is still more states that are not as diverse. This could also be because states in the United States in general aren't super diverse. 

```{R}
# Plot of driver fatalities in each state in the United States
clean_joineddata2 %>% ggplot(aes(x=`Diverse`, y=Premiums))+geom_boxplot()+scale_y_continuous(breaks=seq(0,1400,200))+ggtitle("Insurance Premiums vs. Diverse Areas")+xlab("Diverse Areas")+ylab("Insurance Premium")+geom_jitter(alpha=0.5, aes(color=Uninsured))
```
For the last plot, I wanted to do a boxplot looking at the relationship between the diversity of the areas and the costs of insurance premiums across the state. I also wanted to overly the points with number of people uninsured to look at how these are all related to each other. There seems to be a positive skew for this plot. There also seems to be an outlier for high diversity around 1300 for insurance premiums. In states where there was low diversity, there seemed to be a range of various values of people uninsured and lower median insurance premium than those with high diversity. This could be due to a variety of reasons. In this case, we are only comparing the diversity areas based off of the percentages calculated in the wrangling section; therefore, this could be different if we looked at it from a county point of view. Just as in other boxplots, this plot gives us insight into the visualization of the minimum, first quartile, median, third quartile, and maximum, essentially the summary of the distribution.

#### Concluding Remarks

I really enjoyed looking at these datasets. Keeping up with the news and learning about these incidents in various ways is informing. Before looking at these datasets, I really only focused on data that I saw locally and maybe a couple of nationwide data. I hope that I can continue to be conscious of the ways that I think about the news and how I view them across the states as well. Furthermore, I moved my tidying section to be after the joining becuase my knitting process wouldn't work otherwise. 



